{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Load I-JEPA Checkpoint\n",
        "\n",
        "This notebook loads the I-JEPA model checkpoint and displays its properties.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from pretrain_IJEPA import IJEPA\n",
        "import os\n",
        "from torchviz import make_dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint from checkpoints/small/ijepa-64px-epoch=00.ckpt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\agnish\\anaconda3\\envs\\diffusion\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
          ]
        }
      ],
      "source": [
        "# Path to checkpoint\n",
        "#checkpoint_path = \"lightning_logs/version_0/checkpoints/epoch=9-step=70.ckpt\"\n",
        "checkpoint_path = \"checkpoints/small/ijepa-64px-epoch=00.ckpt\"\n",
        "\n",
        "# Check if checkpoint exists\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    print(f\"Checkpoint not found at {checkpoint_path}\")\n",
        "else:\n",
        "    # Load the model from checkpoint\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    model = IJEPA.load_from_checkpoint(checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "img_size: 64\n",
            "patch_size: 8\n",
            "in_chans: 3\n",
            "embed_dim: 128\n",
            "enc_heads: 4\n",
            "enc_depth: 6\n",
            "decoder_depth: 2\n",
            "lr: 0.001\n",
            "weight_decay: 0.05\n",
            "target_aspect_ratio: (0.75, 1.5)\n",
            "target_scale: (0.15, 0.2)\n",
            "context_aspect_ratio: 1.0\n",
            "context_scale: (0.85, 1.0)\n",
            "M: 4\n",
            "m: 0.996\n",
            "m_start_end: (0.996, 1.0)\n"
          ]
        }
      ],
      "source": [
        "# Print model hyperparameters\n",
        "for key, value in model.hparams.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student Encoder: Encoder\n",
            "Teacher Encoder: Encoder\n",
            "Predictor: Predictor\n"
          ]
        }
      ],
      "source": [
        "# Print model architecture summary\n",
        "print(f\"Student Encoder: {model.model.student_encoder.__class__.__name__}\")\n",
        "print(f\"Teacher Encoder: {model.model.teacher_encoder.__class__.__name__}\")\n",
        "print(f\"Predictor: {model.model.predictor.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 3,716,224\n",
            "Trainable parameters: 3,716,224\n"
          ]
        }
      ],
      "source": [
        "# Print model parameters count\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Additional Model Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of target blocks (M): 4\n",
            "Embedding dimension: 128\n",
            "Patch size: 8\n",
            "Number of tokens: 64\n"
          ]
        }
      ],
      "source": [
        "# Print additional model info\n",
        "print(f\"Number of target blocks (M): {model.M}\")\n",
        "print(f\"Embedding dimension: {model.embed_dim}\")\n",
        "print(f\"Patch size: {model.patch_size}\")\n",
        "print(f\"Number of tokens: {model.num_tokens}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Visualize Model Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IJEPA(\n",
            "  (model): IJEPA_base(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (conv): Conv2d(3, 128, kernel_size=(8, 8), stride=(8, 8))\n",
            "    )\n",
            "    (post_emb_norm): Identity()\n",
            "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (teacher_encoder): Encoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (1): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (2): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (3): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (4): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (5): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (6): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (7): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (8): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (9): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (10): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (11): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "      )\n",
            "      (adaptive_mlp): Identity()\n",
            "      (final_norm): LayerNorm(\n",
            "        (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "      )\n",
            "      (skip_combines): ModuleList(\n",
            "        (0-11): 12 x None\n",
            "      )\n",
            "    )\n",
            "    (student_encoder): Encoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (1): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (2): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (3): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (4): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (5): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (6): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (7): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (8): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (9): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (10): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): Attention(\n",
            "            (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "            (attend): Attend(\n",
            "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "        (11): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): LayerNorm(\n",
            "              (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "            )\n",
            "            (1-2): 2 x None\n",
            "          )\n",
            "          (1): FeedForward(\n",
            "            (ff): Sequential(\n",
            "              (0): Sequential(\n",
            "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (1): GELU(approximate='none')\n",
            "              )\n",
            "              (1): Dropout(p=0.0, inplace=False)\n",
            "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "            )\n",
            "          )\n",
            "          (2): Residual()\n",
            "        )\n",
            "      )\n",
            "      (adaptive_mlp): Identity()\n",
            "      (final_norm): LayerNorm(\n",
            "        (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "      )\n",
            "      (skip_combines): ModuleList(\n",
            "        (0-11): 12 x None\n",
            "      )\n",
            "    )\n",
            "    (predictor): Predictor(\n",
            "      (predictor): Decoder(\n",
            "        (layers): ModuleList(\n",
            "          (0): ModuleList(\n",
            "            (0): ModuleList(\n",
            "              (0): LayerNorm(\n",
            "                (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "              )\n",
            "              (1-2): 2 x None\n",
            "            )\n",
            "            (1): Attention(\n",
            "              (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "              (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "              (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "              (attend): Attend(\n",
            "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "            )\n",
            "            (2): Residual()\n",
            "          )\n",
            "          (1): ModuleList(\n",
            "            (0): ModuleList(\n",
            "              (0): LayerNorm(\n",
            "                (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "              )\n",
            "              (1-2): 2 x None\n",
            "            )\n",
            "            (1): FeedForward(\n",
            "              (ff): Sequential(\n",
            "                (0): Sequential(\n",
            "                  (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Dropout(p=0.0, inplace=False)\n",
            "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "              )\n",
            "            )\n",
            "            (2): Residual()\n",
            "          )\n",
            "          (2): ModuleList(\n",
            "            (0): ModuleList(\n",
            "              (0): LayerNorm(\n",
            "                (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "              )\n",
            "              (1-2): 2 x None\n",
            "            )\n",
            "            (1): Attention(\n",
            "              (to_q): Linear(in_features=128, out_features=256, bias=False)\n",
            "              (to_k): Linear(in_features=128, out_features=256, bias=False)\n",
            "              (to_v): Linear(in_features=128, out_features=256, bias=False)\n",
            "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=4)\n",
            "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
            "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
            "              (attend): Attend(\n",
            "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (to_out): Linear(in_features=256, out_features=128, bias=False)\n",
            "            )\n",
            "            (2): Residual()\n",
            "          )\n",
            "          (3): ModuleList(\n",
            "            (0): ModuleList(\n",
            "              (0): LayerNorm(\n",
            "                (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "              )\n",
            "              (1-2): 2 x None\n",
            "            )\n",
            "            (1): FeedForward(\n",
            "              (ff): Sequential(\n",
            "                (0): Sequential(\n",
            "                  (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Dropout(p=0.0, inplace=False)\n",
            "                (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "              )\n",
            "            )\n",
            "            (2): Residual()\n",
            "          )\n",
            "        )\n",
            "        (adaptive_mlp): Identity()\n",
            "        (final_norm): LayerNorm(\n",
            "          (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
            "        )\n",
            "        (skip_combines): ModuleList(\n",
            "          (0-3): 4 x None\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): MSELoss()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Optional: Print detailed model structure\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Examine Specific Layer Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\agnish\\anaconda3\\envs\\diffusion\\lib\\site-packages\\x_transformers\\attend.py:437: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if n == 1 and causal:\n",
            "c:\\Users\\agnish\\anaconda3\\envs\\diffusion\\lib\\site-packages\\x_transformers\\attend.py:442: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if kv_heads == 1:\n",
            "c:\\Users\\agnish\\anaconda3\\envs\\diffusion\\lib\\site-packages\\x_transformers\\attend.py:444: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  elif kv_heads < heads:\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:91: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  num_patches_block = int(patch_h * patch_w * scale)\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:93: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:93: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:94: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  block_w = int(aspect_ratio * block_h)\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:101: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  start_patch_h = torch.randint(0, patch_h - block_h+1, (1,)).item()\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:102: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  start_patch_w = torch.randint(0, patch_w - block_w+1, (1,)).item()\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:121: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  num_patches_block = int(patch_h * patch_w * scale)\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:123: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:123: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:124: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  block_w = int(aspect_ratio * block_h)\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:126: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  start_patch_h = torch.randint(0, patch_h - block_h+1, (1,)).item()\n",
            "c:\\Users\\agnish\\OneDrive - The University of Chicago\\Projects\\I-JEPA\\model.py:127: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  start_patch_w = torch.randint(0, patch_w - block_w+1, (1,)).item()\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "x = torch.randn((10, 3, 64, 64)).to(device='cuda')  # sequence of token IDs\n",
        "output = model(x, target_aspect_ratio=1, target_scale=1, context_aspect_ratio=1, context_scale=1)\n",
        "#make_dot(output, params=dict(model.named_parameters())).render(\"model\", format=\"png\")\n",
        "torch.onnx.export(model, (x, 1, 1, 1, 1), \"model.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing student and teacher encoder layers:\n",
            "--------------------------------------------------\n",
            "\n",
            "Parameter: 0.0.gamma\n",
            "Mean - Student: -0.049694, Teacher: -0.033852\n",
            "Std  - Student: 0.027912, Teacher: 0.021492\n",
            "Cosine similarity: 0.996385\n",
            "L2 distance: 0.196530\n",
            "\n",
            "Parameter: 1.to_q.weight\n",
            "Mean - Student: 0.000428, Teacher: 0.000431\n",
            "Std  - Student: 0.054726, Teacher: 0.053226\n",
            "Cosine similarity: 0.990926\n",
            "L2 distance: 1.343833\n",
            "\n",
            "Parameter: 1.to_k.weight\n",
            "Mean - Student: 0.000233, Teacher: 0.000220\n",
            "Std  - Student: 0.054352, Teacher: 0.053042\n",
            "Cosine similarity: 0.991540\n",
            "L2 distance: 1.286318\n",
            "\n",
            "Parameter: 1.to_v.weight\n",
            "Mean - Student: 0.000091, Teacher: 0.000099\n",
            "Std  - Student: 0.048634, Teacher: 0.049313\n",
            "Cosine similarity: 0.996382\n",
            "L2 distance: 0.763996\n",
            "\n",
            "Parameter: 1.to_out.weight\n",
            "Mean - Student: -0.000060, Teacher: -0.000053\n",
            "Std  - Student: 0.035202, Teacher: 0.035188\n",
            "Cosine similarity: 0.990941\n",
            "L2 distance: 0.857519\n"
          ]
        }
      ],
      "source": [
        "# Compare with teacher encoder's corresponding layer\n",
        "teacher_layer = model.model.teacher_encoder.layers[0]\n",
        "\n",
        "print(\"Comparing student and teacher encoder layers:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Get state dicts for both layers\n",
        "student_state = specific_layer.state_dict()\n",
        "teacher_state = teacher_layer.state_dict()\n",
        "\n",
        "# Compare parameters for each key in state dict\n",
        "for key in student_state.keys():\n",
        "    if key in teacher_state:\n",
        "        student_param = student_state[key]\n",
        "        teacher_param = teacher_state[key]\n",
        "        \n",
        "        if student_param.shape == teacher_param.shape:\n",
        "            # Calculate statistics\n",
        "            student_mean = student_param.mean().item()\n",
        "            teacher_mean = teacher_param.mean().item()\n",
        "            student_std = student_param.std().item()\n",
        "            teacher_std = teacher_param.std().item()\n",
        "            \n",
        "            print(f\"\\nParameter: {key}\")\n",
        "            print(f\"Mean - Student: {student_mean:.6f}, Teacher: {teacher_mean:.6f}\")\n",
        "            print(f\"Std  - Student: {student_std:.6f}, Teacher: {teacher_std:.6f}\")\n",
        "            \n",
        "            # Calculate similarity metrics\n",
        "            # Cosine similarity\n",
        "            student_flat = student_param.flatten()\n",
        "            teacher_flat = teacher_param.flatten()\n",
        "            cos_sim = torch.nn.functional.cosine_similarity(student_flat, teacher_flat, dim=0)\n",
        "            print(f\"Cosine similarity: {cos_sim.item():.6f}\")\n",
        "            \n",
        "            # L2 distance\n",
        "            l2_dist = torch.norm(student_param - teacher_param).item()\n",
        "            print(f\"L2 distance: {l2_dist:.6f}\")\n",
        "        else:\n",
        "            print(f\"\\nParameter: {key}\")\n",
        "            print(f\"Shapes differ - Student: {student_param.shape}, Teacher: {teacher_param.shape}\")\n",
        "    else:\n",
        "        print(f\"\\nParameter {key} exists in student but not in teacher\")\n",
        "\n",
        "# Check for parameters in teacher but not in student\n",
        "for key in teacher_state.keys():\n",
        "    if key not in student_state:\n",
        "        print(f\"\\nParameter {key} exists in teacher but not in student\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing student and teacher encoder layers:\n",
            "--------------------------------------------------\n",
            "\n",
            "Parameter: 0.0.gamma\n",
            "Mean - Student: -0.049694, Teacher: -0.033852\n",
            "Std  - Student: 0.027912, Teacher: 0.021492\n",
            "Cosine similarity: 0.996385\n",
            "L2 distance: 0.196530\n",
            "\n",
            "Parameter: 1.to_q.weight\n",
            "Mean - Student: 0.000428, Teacher: 0.000431\n",
            "Std  - Student: 0.054726, Teacher: 0.053226\n",
            "Cosine similarity: 0.990926\n",
            "L2 distance: 1.343833\n",
            "\n",
            "Parameter: 1.to_k.weight\n",
            "Mean - Student: 0.000233, Teacher: 0.000220\n",
            "Std  - Student: 0.054352, Teacher: 0.053042\n",
            "Cosine similarity: 0.991540\n",
            "L2 distance: 1.286318\n",
            "\n",
            "Parameter: 1.to_v.weight\n",
            "Mean - Student: 0.000091, Teacher: 0.000099\n",
            "Std  - Student: 0.048634, Teacher: 0.049313\n",
            "Cosine similarity: 0.996382\n",
            "L2 distance: 0.763996\n",
            "\n",
            "Parameter: 1.to_out.weight\n",
            "Mean - Student: -0.000060, Teacher: -0.000053\n",
            "Std  - Student: 0.035202, Teacher: 0.035188\n",
            "Cosine similarity: 0.990941\n",
            "L2 distance: 0.857519\n"
          ]
        }
      ],
      "source": [
        "# Compare with teacher encoder's corresponding layer\n",
        "teacher_layer = model.model.teacher_encoder.layers[0]\n",
        "\n",
        "print(\"Comparing student and teacher encoder layers:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Get state dicts for both layers\n",
        "student_state = specific_layer.state_dict()\n",
        "teacher_state = teacher_layer.state_dict()\n",
        "\n",
        "# Compare parameters for each key in state dict\n",
        "for key in student_state.keys():\n",
        "    if key in teacher_state:\n",
        "        student_param = student_state[key]\n",
        "        teacher_param = teacher_state[key]\n",
        "        \n",
        "        if student_param.shape == teacher_param.shape:\n",
        "            # Calculate statistics\n",
        "            student_mean = student_param.mean().item()\n",
        "            teacher_mean = teacher_param.mean().item()\n",
        "            student_std = student_param.std().item()\n",
        "            teacher_std = teacher_param.std().item()\n",
        "            \n",
        "            print(f\"\\nParameter: {key}\")\n",
        "            print(f\"Mean - Student: {student_mean:.6f}, Teacher: {teacher_mean:.6f}\")\n",
        "            print(f\"Std  - Student: {student_std:.6f}, Teacher: {teacher_std:.6f}\")\n",
        "            \n",
        "            # Calculate similarity metrics\n",
        "            # Cosine similarity\n",
        "            student_flat = student_param.flatten()\n",
        "            teacher_flat = teacher_param.flatten()\n",
        "            cos_sim = torch.nn.functional.cosine_similarity(student_flat, teacher_flat, dim=0)\n",
        "            print(f\"Cosine similarity: {cos_sim.item():.6f}\")\n",
        "            \n",
        "            # L2 distance\n",
        "            l2_dist = torch.norm(student_param - teacher_param).item()\n",
        "            print(f\"L2 distance: {l2_dist:.6f}\")\n",
        "        else:\n",
        "            print(f\"\\nParameter: {key}\")\n",
        "            print(f\"Shapes differ - Student: {student_param.shape}, Teacher: {teacher_param.shape}\")\n",
        "    else:\n",
        "        print(f\"\\nParameter {key} exists in student but not in teacher\")\n",
        "\n",
        "# Check for parameters in teacher but not in student\n",
        "for key in teacher_state.keys():\n",
        "    if key not in student_state:\n",
        "        print(f\"\\nParameter {key} exists in teacher but not in student\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer type: ModuleList\n",
            "Layer attributes: ['T_destination', '__add__', '__annotations__', '__call__', '__class__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_abs_string_index', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'append', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extend', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'insert', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'pop', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
            "Layer state_dict keys: odict_keys(['0.0.gamma', '1.to_q.weight', '1.to_k.weight', '1.to_v.weight', '1.to_out.weight'])\n",
            "\n",
            "Parameters:\n",
            "  - 0.0.gamma: torch.Size([128])\n",
            "  - 1.to_q.weight: torch.Size([256, 128])\n",
            "  - 1.to_k.weight: torch.Size([256, 128])\n",
            "  - 1.to_v.weight: torch.Size([256, 128])\n",
            "  - 1.to_out.weight: torch.Size([128, 256])\n",
            "\n",
            "Buffers:\n",
            "\n",
            "State Dict:\n",
            "  - 0.0.gamma: torch.Size([128])\n",
            "    First few values: tensor([-0.0372, -0.0379, -0.0321, -0.0477, -0.0707], device='cuda:0')\n",
            "    Mean: -0.049694, Std: 0.027912\n",
            "  - 1.to_q.weight: torch.Size([256, 128])\n",
            "    First few values: tensor([-0.0906, -0.0152,  0.0484, -0.0309, -0.0403], device='cuda:0')\n",
            "    Mean: 0.000428, Std: 0.054726\n",
            "  - 1.to_k.weight: torch.Size([256, 128])\n",
            "    First few values: tensor([ 0.0529,  0.0466, -0.0884, -0.0748, -0.0766], device='cuda:0')\n",
            "    Mean: 0.000233, Std: 0.054352\n",
            "  - 1.to_v.weight: torch.Size([256, 128])\n",
            "    First few values: tensor([-0.0609,  0.0511, -0.0436,  0.0840,  0.0508], device='cuda:0')\n",
            "    Mean: 0.000091, Std: 0.048634\n",
            "  - 1.to_out.weight: torch.Size([128, 256])\n",
            "    First few values: tensor([-0.0055,  0.0272, -0.0443, -0.0169, -0.0178], device='cuda:0')\n",
            "    Mean: -0.000060, Std: 0.035202\n",
            "\n",
            "Weight shape: torch.Size([128, 256])\n",
            "Weight values (first 10):\n",
            "tensor([[-0.0055,  0.0272, -0.0443,  ..., -0.0432, -0.0468,  0.0093],\n",
            "        [ 0.0110, -0.0100, -0.0545,  ...,  0.0160,  0.0300,  0.0531],\n",
            "        [ 0.0375,  0.0341, -0.0272,  ..., -0.0439,  0.0036,  0.0060],\n",
            "        ...,\n",
            "        [-0.0280,  0.0213,  0.0880,  ...,  0.0188, -0.0199,  0.0023],\n",
            "        [-0.0333,  0.0543, -0.0231,  ...,  0.0443, -0.0029, -0.0488],\n",
            "        [-0.0686,  0.0257,  0.0086,  ...,  0.0301, -0.0547, -0.0384]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "Weight statistics:\n",
            "  Mean: -0.000060\n",
            "  Std: 0.035202\n",
            "  Min: -0.097598\n",
            "  Max: 0.111274\n",
            "\n",
            "No bias parameter found in this layer\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Access the specific layer: model.model.student_encoder.layers[0][0][0].ln\n",
        "specific_layer = model.model.student_encoder.layers[0]\n",
        "\n",
        "# Display layer type and structure\n",
        "print(f\"Layer type: {type(specific_layer).__name__}\")\n",
        "print(f\"Layer attributes: {dir(specific_layer)}\")\n",
        "print(f\"Layer state_dict keys: {specific_layer.state_dict().keys()}\")\n",
        "\n",
        "# Inspect the layer's parameters\n",
        "print(\"\\nParameters:\")\n",
        "for name, param in specific_layer.named_parameters():\n",
        "    print(f\"  - {name}: {param.shape}\")\n",
        "\n",
        "# Inspect the layer's buffers\n",
        "print(\"\\nBuffers:\")\n",
        "for name, buffer in specific_layer.named_buffers():\n",
        "    print(f\"  - {name}: {buffer.shape}\")\n",
        "\n",
        "# Inspect the state dict\n",
        "print(\"\\nState Dict:\")\n",
        "for key, value in specific_layer.state_dict().items():\n",
        "    print(f\"  - {key}: {value.shape}\")\n",
        "    print(f\"    First few values: {value.flatten()[:5]}\")\n",
        "    print(f\"    Mean: {value.mean().item():.6f}, Std: {value.std().item():.6f}\")\n",
        "\n",
        "# Safer way to display weights and bias\n",
        "weight = None\n",
        "bias = None\n",
        "\n",
        "for name, param in specific_layer.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        weight = param\n",
        "    elif 'bias' in name:\n",
        "        bias = param\n",
        "\n",
        "# If we didn't find them in parameters, check state_dict\n",
        "if weight is None and 'weight' in specific_layer.state_dict():\n",
        "    weight = specific_layer.state_dict()['weight']\n",
        "if bias is None and 'bias' in specific_layer.state_dict():\n",
        "    bias = specific_layer.state_dict()['bias']\n",
        "\n",
        "# Now display weight info if found\n",
        "if weight is not None:\n",
        "    print(\"\\nWeight shape:\", weight.shape)\n",
        "    print(f\"Weight values (first 10):\\n{weight[:10]}\")\n",
        "    print(f\"Weight statistics:\")\n",
        "    print(f\"  Mean: {weight.mean().item():.6f}\")\n",
        "    print(f\"  Std: {weight.std().item():.6f}\")\n",
        "    print(f\"  Min: {weight.min().item():.6f}\")\n",
        "    print(f\"  Max: {weight.max().item():.6f}\")\n",
        "else:\n",
        "    print(\"\\nNo weight parameter found in this layer\")\n",
        "\n",
        "# Display bias info if found\n",
        "if bias is not None:\n",
        "    print(\"\\nBias shape:\", bias.shape)\n",
        "    print(f\"Bias values (first 10):\\n{bias[:10]}\")\n",
        "    print(f\"Bias statistics:\")\n",
        "    print(f\"  Mean: {bias.mean().item():.6f}\")\n",
        "    print(f\"  Std: {bias.std().item():.6f}\")\n",
        "    print(f\"  Min: {bias.min().item():.6f}\")\n",
        "    print(f\"  Max: {bias.max().item():.6f}\")\n",
        "else:\n",
        "    print(\"\\nNo bias parameter found in this layer\")\n",
        "\n",
        "# Display histogram of weights\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    if weight is not None:\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.hist(weight.detach().flatten().cpu().numpy(), bins=50)\n",
        "        plt.title('Weight Distribution')\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Frequency')\n",
        "    \n",
        "    if bias is not None:\n",
        "        plt.subplot(1, 2, 2 if weight is not None else 1, 1)\n",
        "        plt.hist(bias.detach().flatten().cpu().numpy(), bins=50)\n",
        "        plt.title('Bias Distribution')\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Frequency')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except ImportError:\n",
        "    print(\"Matplotlib not available for histogram visualization\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating histogram: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Save Layer Weights to File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0.gamma saved to student_encoder_layer0_ln_0.0.gamma.npy\n",
            "1.to_q.weight saved to student_encoder_layer0_ln_1.to_q.weight.npy\n",
            "1.to_k.weight saved to student_encoder_layer0_ln_1.to_k.weight.npy\n",
            "1.to_v.weight saved to student_encoder_layer0_ln_1.to_v.weight.npy\n",
            "1.to_out.weight saved to student_encoder_layer0_ln_1.to_out.weight.npy\n"
          ]
        }
      ],
      "source": [
        "# Save the weights to a file for further analysis\n",
        "import numpy as np\n",
        "\n",
        "# Function to save layer weights\n",
        "def save_layer_weights(layer, filename_prefix=\"layer_weights\"):\n",
        "    \"\"\"Save layer weights and bias to numpy files\"\"\"\n",
        "    # Get weights and biases from state_dict\n",
        "    state_dict = layer.state_dict()\n",
        "    \n",
        "    for key, value in state_dict.items():\n",
        "        if value is not None:\n",
        "            np_value = value.detach().cpu().numpy()\n",
        "            np.save(f\"{filename_prefix}_{key}.npy\", np_value)\n",
        "            print(f\"{key} saved to {filename_prefix}_{key}.npy\")\n",
        "\n",
        "# Save the specific layer weights\n",
        "save_layer_weights(specific_layer, \"student_encoder_layer0_ln\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Compare with Other Layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing student and teacher encoder layers:\n",
            "--------------------------------------------------\n",
            "\n",
            "Parameter: 0.0.gamma\n",
            "Mean - Student: -0.049694, Teacher: -0.033852\n",
            "Std  - Student: 0.027912, Teacher: 0.021492\n",
            "Cosine similarity: 0.996385\n",
            "L2 distance: 0.196530\n",
            "\n",
            "Parameter: 1.to_q.weight\n",
            "Mean - Student: 0.000428, Teacher: 0.000431\n",
            "Std  - Student: 0.054726, Teacher: 0.053226\n",
            "Cosine similarity: 0.990926\n",
            "L2 distance: 1.343833\n",
            "\n",
            "Parameter: 1.to_k.weight\n",
            "Mean - Student: 0.000233, Teacher: 0.000220\n",
            "Std  - Student: 0.054352, Teacher: 0.053042\n",
            "Cosine similarity: 0.991540\n",
            "L2 distance: 1.286318\n",
            "\n",
            "Parameter: 1.to_v.weight\n",
            "Mean - Student: 0.000091, Teacher: 0.000099\n",
            "Std  - Student: 0.048634, Teacher: 0.049313\n",
            "Cosine similarity: 0.996382\n",
            "L2 distance: 0.763996\n",
            "\n",
            "Parameter: 1.to_out.weight\n",
            "Mean - Student: -0.000060, Teacher: -0.000053\n",
            "Std  - Student: 0.035202, Teacher: 0.035188\n",
            "Cosine similarity: 0.990941\n",
            "L2 distance: 0.857519\n"
          ]
        }
      ],
      "source": [
        "# Compare with teacher encoder's corresponding layer\n",
        "teacher_layer = model.model.teacher_encoder.layers[0]\n",
        "\n",
        "print(\"Comparing student and teacher encoder layers:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Get state dicts for both layers\n",
        "student_state = specific_layer.state_dict()\n",
        "teacher_state = teacher_layer.state_dict()\n",
        "\n",
        "# Compare parameters for each key in state dict\n",
        "for key in student_state.keys():\n",
        "    if key in teacher_state:\n",
        "        student_param = student_state[key]\n",
        "        teacher_param = teacher_state[key]\n",
        "        \n",
        "        if student_param.shape == teacher_param.shape:\n",
        "            # Calculate statistics\n",
        "            student_mean = student_param.mean().item()\n",
        "            teacher_mean = teacher_param.mean().item()\n",
        "            student_std = student_param.std().item()\n",
        "            teacher_std = teacher_param.std().item()\n",
        "            \n",
        "            print(f\"\\nParameter: {key}\")\n",
        "            print(f\"Mean - Student: {student_mean:.6f}, Teacher: {teacher_mean:.6f}\")\n",
        "            print(f\"Std  - Student: {student_std:.6f}, Teacher: {teacher_std:.6f}\")\n",
        "            \n",
        "            # Calculate similarity metrics\n",
        "            # Cosine similarity\n",
        "            student_flat = student_param.flatten()\n",
        "            teacher_flat = teacher_param.flatten()\n",
        "            cos_sim = torch.nn.functional.cosine_similarity(student_flat, teacher_flat, dim=0)\n",
        "            print(f\"Cosine similarity: {cos_sim.item():.6f}\")\n",
        "            \n",
        "            # L2 distance\n",
        "            l2_dist = torch.norm(student_param - teacher_param).item()\n",
        "            print(f\"L2 distance: {l2_dist:.6f}\")\n",
        "        else:\n",
        "            print(f\"\\nParameter: {key}\")\n",
        "            print(f\"Shapes differ - Student: {student_param.shape}, Teacher: {teacher_param.shape}\")\n",
        "    else:\n",
        "        print(f\"\\nParameter {key} exists in student but not in teacher\")\n",
        "\n",
        "# Check for parameters in teacher but not in student\n",
        "for key in teacher_state.keys():\n",
        "    if key not in student_state:\n",
        "        print(f\"\\nParameter {key} exists in teacher but not in student\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing student and teacher encoder layers:\n",
            "--------------------------------------------------\n",
            "\n",
            "Parameter: 0.0.gamma\n",
            "Mean - Student: -0.049694, Teacher: -0.033852\n",
            "Std  - Student: 0.027912, Teacher: 0.021492\n",
            "Cosine similarity: 0.996385\n",
            "L2 distance: 0.196530\n",
            "\n",
            "Parameter: 1.to_q.weight\n",
            "Mean - Student: 0.000428, Teacher: 0.000431\n",
            "Std  - Student: 0.054726, Teacher: 0.053226\n",
            "Cosine similarity: 0.990926\n",
            "L2 distance: 1.343833\n",
            "\n",
            "Parameter: 1.to_k.weight\n",
            "Mean - Student: 0.000233, Teacher: 0.000220\n",
            "Std  - Student: 0.054352, Teacher: 0.053042\n",
            "Cosine similarity: 0.991540\n",
            "L2 distance: 1.286318\n",
            "\n",
            "Parameter: 1.to_v.weight\n",
            "Mean - Student: 0.000091, Teacher: 0.000099\n",
            "Std  - Student: 0.048634, Teacher: 0.049313\n",
            "Cosine similarity: 0.996382\n",
            "L2 distance: 0.763996\n",
            "\n",
            "Parameter: 1.to_out.weight\n",
            "Mean - Student: -0.000060, Teacher: -0.000053\n",
            "Std  - Student: 0.035202, Teacher: 0.035188\n",
            "Cosine similarity: 0.990941\n",
            "L2 distance: 0.857519\n"
          ]
        }
      ],
      "source": [
        "# Compare with teacher encoder's corresponding layer\n",
        "teacher_layer = model.model.teacher_encoder.layers[0]\n",
        "\n",
        "print(\"Comparing student and teacher encoder layers:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Get state dicts for both layers\n",
        "student_state = specific_layer.state_dict()\n",
        "teacher_state = teacher_layer.state_dict()\n",
        "\n",
        "# Compare parameters for each key in state dict\n",
        "for key in student_state.keys():\n",
        "    if key in teacher_state:\n",
        "        student_param = student_state[key]\n",
        "        teacher_param = teacher_state[key]\n",
        "        \n",
        "        if student_param.shape == teacher_param.shape:\n",
        "            # Calculate statistics\n",
        "            student_mean = student_param.mean().item()\n",
        "            teacher_mean = teacher_param.mean().item()\n",
        "            student_std = student_param.std().item()\n",
        "            teacher_std = teacher_param.std().item()\n",
        "            \n",
        "            print(f\"\\nParameter: {key}\")\n",
        "            print(f\"Mean - Student: {student_mean:.6f}, Teacher: {teacher_mean:.6f}\")\n",
        "            print(f\"Std  - Student: {student_std:.6f}, Teacher: {teacher_std:.6f}\")\n",
        "            \n",
        "            # Calculate similarity metrics\n",
        "            # Cosine similarity\n",
        "            student_flat = student_param.flatten()\n",
        "            teacher_flat = teacher_param.flatten()\n",
        "            cos_sim = torch.nn.functional.cosine_similarity(student_flat, teacher_flat, dim=0)\n",
        "            print(f\"Cosine similarity: {cos_sim.item():.6f}\")\n",
        "            \n",
        "            # L2 distance\n",
        "            l2_dist = torch.norm(student_param - teacher_param).item()\n",
        "            print(f\"L2 distance: {l2_dist:.6f}\")\n",
        "        else:\n",
        "            print(f\"\\nParameter: {key}\")\n",
        "            print(f\"Shapes differ - Student: {student_param.shape}, Teacher: {teacher_param.shape}\")\n",
        "    else:\n",
        "        print(f\"\\nParameter {key} exists in student but not in teacher\")\n",
        "\n",
        "# Check for parameters in teacher but not in student\n",
        "for key in teacher_state.keys():\n",
        "    if key not in student_state:\n",
        "        print(f\"\\nParameter {key} exists in teacher but not in student\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing student and teacher encoder layers:\n",
            "--------------------------------------------------\n",
            "\n",
            "Parameter: 0.0.gamma\n",
            "Mean - Student: -0.049694, Teacher: -0.033852\n",
            "Std  - Student: 0.027912, Teacher: 0.021492\n",
            "Cosine similarity: 0.996385\n",
            "L2 distance: 0.196530\n",
            "\n",
            "Parameter: 1.to_q.weight\n",
            "Mean - Student: 0.000428, Teacher: 0.000431\n",
            "Std  - Student: 0.054726, Teacher: 0.053226\n",
            "Cosine similarity: 0.990926\n",
            "L2 distance: 1.343833\n",
            "\n",
            "Parameter: 1.to_k.weight\n",
            "Mean - Student: 0.000233, Teacher: 0.000220\n",
            "Std  - Student: 0.054352, Teacher: 0.053042\n",
            "Cosine similarity: 0.991540\n",
            "L2 distance: 1.286318\n",
            "\n",
            "Parameter: 1.to_v.weight\n",
            "Mean - Student: 0.000091, Teacher: 0.000099\n",
            "Std  - Student: 0.048634, Teacher: 0.049313\n",
            "Cosine similarity: 0.996382\n",
            "L2 distance: 0.763996\n",
            "\n",
            "Parameter: 1.to_out.weight\n",
            "Mean - Student: -0.000060, Teacher: -0.000053\n",
            "Std  - Student: 0.035202, Teacher: 0.035188\n",
            "Cosine similarity: 0.990941\n",
            "L2 distance: 0.857519\n"
          ]
        }
      ],
      "source": [
        "# Compare with teacher encoder's corresponding layer\n",
        "teacher_layer = model.model.teacher_encoder.layers[0]\n",
        "\n",
        "print(\"Comparing student and teacher encoder layers:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Get state dicts for both layers\n",
        "student_state = specific_layer.state_dict()\n",
        "teacher_state = teacher_layer.state_dict()\n",
        "\n",
        "# Compare parameters for each key in state dict\n",
        "for key in student_state.keys():\n",
        "    if key in teacher_state:\n",
        "        student_param = student_state[key]\n",
        "        teacher_param = teacher_state[key]\n",
        "        \n",
        "        if student_param.shape == teacher_param.shape:\n",
        "            # Calculate statistics\n",
        "            student_mean = student_param.mean().item()\n",
        "            teacher_mean = teacher_param.mean().item()\n",
        "            student_std = student_param.std().item()\n",
        "            teacher_std = teacher_param.std().item()\n",
        "            \n",
        "            print(f\"\\nParameter: {key}\")\n",
        "            print(f\"Mean - Student: {student_mean:.6f}, Teacher: {teacher_mean:.6f}\")\n",
        "            print(f\"Std  - Student: {student_std:.6f}, Teacher: {teacher_std:.6f}\")\n",
        "            \n",
        "            # Calculate similarity metrics\n",
        "            # Cosine similarity\n",
        "            student_flat = student_param.flatten()\n",
        "            teacher_flat = teacher_param.flatten()\n",
        "            cos_sim = torch.nn.functional.cosine_similarity(student_flat, teacher_flat, dim=0)\n",
        "            print(f\"Cosine similarity: {cos_sim.item():.6f}\")\n",
        "            \n",
        "            # L2 distance\n",
        "            l2_dist = torch.norm(student_param - teacher_param).item()\n",
        "            print(f\"L2 distance: {l2_dist:.6f}\")\n",
        "        else:\n",
        "            print(f\"\\nParameter: {key}\")\n",
        "            print(f\"Shapes differ - Student: {student_param.shape}, Teacher: {teacher_param.shape}\")\n",
        "    else:\n",
        "        print(f\"\\nParameter {key} exists in student but not in teacher\")\n",
        "\n",
        "# Check for parameters in teacher but not in student\n",
        "for key in teacher_state.keys():\n",
        "    if key not in student_state:\n",
        "        print(f\"\\nParameter {key} exists in teacher but not in student\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Explore Layer Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exploring model.model.student_encoder.layers[0][0][0].ln structure:\n",
            "Layer class: LayerNorm\n",
            "Layer module path: torch.nn.modules.normalization\n",
            "\n",
            "This is a standard PyTorch LayerNorm layer\n",
            "Normalized shape: (64,)\n",
            "Epsilon: 1e-05\n",
            "Element-wise affine: False\n",
            "\n",
            "Direct access to _parameters dictionary:\n",
            "  weight: None\n",
            "  bias: None\n",
            "\n",
            "Registered buffers:\n",
            "\n",
            "Module children:\n",
            "  No children modules\n",
            "\n",
            "Full layer representation:\n",
            "LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n"
          ]
        }
      ],
      "source": [
        "# Let's explore the structure of the model to understand how to access layer weights\n",
        "print(\"Exploring model.model.student_encoder.layers[0][0][0].ln structure:\")\n",
        "\n",
        "# First, check what type of layer this is\n",
        "layer = model.model.student_encoder.layers[0][0][0].ln\n",
        "print(f\"Layer class: {layer.__class__.__name__}\")\n",
        "print(f\"Layer module path: {layer.__class__.__module__}\")\n",
        "\n",
        "# Check if this is a PyTorch LayerNorm\n",
        "if isinstance(layer, torch.nn.LayerNorm):\n",
        "    print(\"\\nThis is a standard PyTorch LayerNorm layer\")\n",
        "    print(f\"Normalized shape: {layer.normalized_shape}\")\n",
        "    print(f\"Epsilon: {layer.eps}\")\n",
        "    print(f\"Element-wise affine: {layer.elementwise_affine}\")\n",
        "    \n",
        "    # Check if weights are registered as parameters\n",
        "    for name, param in layer.named_parameters():\n",
        "        print(f\"\\nParameter: {name}\")\n",
        "        print(f\"  Shape: {param.shape}\")\n",
        "        print(f\"  Requires grad: {param.requires_grad}\")\n",
        "        print(f\"  First few values: {param.data.flatten()[:5]}\")\n",
        "        \n",
        "# Try to access _parameters dictionary directly\n",
        "print(\"\\nDirect access to _parameters dictionary:\")\n",
        "if hasattr(layer, '_parameters'):\n",
        "    for name, param in layer._parameters.items():\n",
        "        if param is not None:\n",
        "            print(f\"  {name}: {param.shape}\")\n",
        "        else:\n",
        "            print(f\"  {name}: None\")\n",
        "else:\n",
        "    print(\"  No _parameters attribute found\")\n",
        "\n",
        "# Check if there are any registered buffers\n",
        "print(\"\\nRegistered buffers:\")\n",
        "if hasattr(layer, '_buffers'):\n",
        "    for name, buffer in layer._buffers.items():\n",
        "        if buffer is not None:\n",
        "            print(f\"  {name}: {buffer.shape}\")\n",
        "        else:\n",
        "            print(f\"  {name}: None\")\n",
        "else:\n",
        "    print(\"  No _buffers attribute found\")\n",
        "\n",
        "# Print the module's children\n",
        "print(\"\\nModule children:\")\n",
        "has_children = False\n",
        "for i, child in enumerate(layer.children()):\n",
        "    has_children = True\n",
        "    print(f\"  Child {i}: {child.__class__.__name__}\")\n",
        "if not has_children:\n",
        "    print(\"  No children modules\")\n",
        "\n",
        "# Print the full repr of the layer\n",
        "print(\"\\nFull layer representation:\")\n",
        "print(layer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Investigate Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found IJEPA_base class in model module\n",
            "\n",
            "Class definition:\n",
            "class IJEPA_base(nn.Module):\n",
            "    def __init__(self, img_size, patch_size, in_chans, embed_dim, enc_depth, pred_depth, num_heads, post_emb_norm=False, M = 4, mode=\"train\", layer_dropout=0.):\n",
            "        super().__init__()\n",
            "        self.M = M\n",
            "        self.mode = mode\n",
            "        self.layer_dropout = layer_dropout\n",
            "\n",
            "        #define the patch embedding and positional embedding\n",
            "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
            "        self.patch_dim  = (self.patch_embed.patch_shape[0], self.patch_embed.patch_shape[1])\n",
            "        self.num_tokens = self.patch_embed.patch_shape[0] * self.patch_embed.patch_shape[1]\n",
            "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, embed_dim))\n",
            "\n",
            "        #define the cls and mask tokens\n",
            "        self.mask_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
            "        nn.init.trunc_normal_(self.mask_token, 0.02)\n",
            "\n",
            "        #define the encoder and decoder, as well as the layer normalization and dropout\n",
            "        self.post_emb_norm = nn.LayerNorm(embed_dim) if post_emb_norm else nn.Identity()\n",
            "        self.norm = nn.LayerNorm(embed_dim)\n",
            "        self.teacher_encoder = Encoder(\n",
            "            dim=embed_dim,\n",
            "            heads=num_heads,\n",
            "            depth=enc_depth, \n",
            "            layer_dropout=self.layer_dropout,\n",
            "        )  \n",
            "        self.student_encoder = copy.deepcopy(self.teacher_encoder).cuda()\n",
            "        self.predictor = Predictor(embed_dim, num_heads, pred_depth)\n",
            "\n",
            "    @torch.no_grad() \n",
            "    def get_target_block(self, target_encoder, x, patch_dim, aspect_ratio, scale, M):  \n",
            "        #get the target block\n",
            "        target_encoder = target_encoder.eval()\n",
            "        x = target_encoder(x)\n",
            "        x = self.norm(x)\n",
            "        #get the patch dimensions\n",
            "        patch_h, patch_w = patch_dim\n",
            "        #get the number of patches\n",
            "        num_patches = patch_h * patch_w\n",
            "        #get the number of patches in the target block\n",
            "        num_patches_block = int(patch_h * patch_w * scale)\n",
            "        #get the height and width of the target block with aspect ratio\n",
            "        block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
            "        block_w = int(aspect_ratio * block_h)\n",
            "        #get the patches in the target block\n",
            "        target_block = torch.zeros((M, x.shape[0], block_h*block_w, x.shape[2]))\n",
            "        target_patches = []\n",
            "        all_patches = []\n",
            "        for z in range(M):\n",
            "            #get the starting patch\n",
            "            start_patch_h = torch.randint(0, patch_h - block_h+1, (1,)).item()\n",
            "            start_patch_w = torch.randint(0, patch_w - block_w+1, (1,)).item()\n",
            "            start_patch = start_patch_h * patch_w + start_patch_w\n",
            "\n",
            "            patches = []\n",
            "            #get the patches in the target block\n",
            "            for i in range(block_h):\n",
            "                for j in range(block_w):\n",
            "                    patches.append(start_patch + i * patch_w + j)\n",
            "                    if start_patch + i * patch_w + j not in all_patches:\n",
            "                        all_patches.append(start_patch + i * patch_w + j)\n",
            "                    \n",
            "            #get the target block\n",
            "            target_patches.append(patches)\n",
            "            target_block[z] = x[:, patches, :]\n",
            "        return target_block.cuda(), target_patches, all_patches\n",
            "\n",
            "    def get_context_block(self, x, patch_dim, aspect_ratio, scale, target_patches):\n",
            "        patch_h, patch_w = patch_dim\n",
            "        #get the number of patches in the target block\n",
            "        num_patches_block = int(patch_h * patch_w * scale)\n",
            "        #get the height and width of the target block with aspect ratio\n",
            "        block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
            "        block_w = int(aspect_ratio * block_h)\n",
            "        #get the starting patch\n",
            "        start_patch_h = torch.randint(0, patch_h - block_h+1, (1,)).item()\n",
            "        start_patch_w = torch.randint(0, patch_w - block_w+1, (1,)).item()\n",
            "        start_patch = start_patch_h * patch_w + start_patch_w\n",
            "        #get the patches in the context_block\n",
            "        patches = []\n",
            "        for i in range(block_h):\n",
            "            for j in range(block_w):\n",
            "                if start_patch + i * patch_w + j not in target_patches: #remove the target patches\n",
            "                    patches.append(start_patch + i * patch_w + j)\n",
            "        return x[:, patches, :]\n",
            "\n",
            "\n",
            "    def forward(self, x, target_aspect_ratio=1, target_scale=1, context_aspect_ratio=1, context_scale=1):\n",
            "        #get the patch embeddings\n",
            "        x = self.patch_embed(x)\n",
            "        b, n, e = x.shape\n",
            "        #add the positional embeddings\n",
            "        x = x + self.pos_embedding\n",
            "        #normalize the embeddings\n",
            "        x = self.post_emb_norm(x)\n",
            "        #if mode is test, we get return full embedding:\n",
            "        if self.mode == 'test':\n",
            "            return self.student_encoder(x)\n",
            "        # #get target embeddings\n",
            "        target_blocks, target_patches, all_patches = self.get_target_block(self.teacher_encoder, x, self.patch_dim, target_aspect_ratio, target_scale, self.M)\n",
            "        m, b, n, e = target_blocks.shape\n",
            "        #get context embedding\n",
            "\n",
            "        context_block = self.get_context_block(x, self.patch_dim, context_aspect_ratio, context_scale, all_patches)\n",
            "        context_encoding = self.student_encoder(context_block)\n",
            "        context_encoding = self.norm(context_encoding)\n",
            "\n",
            "\n",
            "        prediction_blocks = torch.zeros((m, b, n, e)).cuda()\n",
            "        #get the prediction blocks, predict each target block separately\n",
            "        for i in range(m):\n",
            "            target_masks = self.mask_token.repeat(b, n, 1)\n",
            "            target_pos_embedding = self.pos_embedding[:, target_patches[i], :]\n",
            "            target_masks = target_masks + target_pos_embedding\n",
            "            prediction_blocks[i] = self.predictor(context_encoding, target_masks)\n",
            "\n",
            "        return prediction_blocks, target_blocks\n",
            "\n",
            "\n",
            "All classes in model module:\n",
            "  - Decoder\n",
            "  - Encoder\n",
            "  - IJEPA_base\n",
            "  - PatchEmbed\n",
            "  - Predictor\n",
            "\n",
            "Student Encoder Structure:\n",
            "Type: Encoder\n",
            "\n",
            "Layers structure:\n",
            "Type of layers: ModuleList\n",
            "Length of layers: 16\n",
            "\n",
            "First layer structure:\n",
            "Type of first layer: ModuleList\n",
            "Length of first layer: 3\n",
            "\n",
            "Layers[0][0] structure:\n",
            "Type: ModuleList\n",
            "Length: 3\n",
            "\n",
            "Layers[0][0][0] structure:\n",
            "  - ln: LayerNorm\n",
            "\n",
            "LayerNorm elementwise_affine: False\n"
          ]
        }
      ],
      "source": [
        "# Let's look at the model.py file to understand the architecture\n",
        "import inspect\n",
        "\n",
        "# Check if we can import the model module\n",
        "try:\n",
        "    import model as model_module\n",
        "    \n",
        "    # Get the source code of the IJEPA_base class\n",
        "    if hasattr(model_module, 'IJEPA_base'):\n",
        "        print(\"Found IJEPA_base class in model module\")\n",
        "        print(\"\\nClass definition:\")\n",
        "        print(inspect.getsource(model_module.IJEPA_base))\n",
        "    else:\n",
        "        print(\"IJEPA_base class not found in model module\")\n",
        "        \n",
        "    # List all classes in the module\n",
        "    print(\"\\nAll classes in model module:\")\n",
        "    for name, obj in inspect.getmembers(model_module, inspect.isclass):\n",
        "        print(f\"  - {name}\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"Could not import model module\")\n",
        "    \n",
        "# Let's also check the structure of the student encoder\n",
        "student_encoder = model.model.student_encoder\n",
        "print(\"\\nStudent Encoder Structure:\")\n",
        "print(f\"Type: {type(student_encoder).__name__}\")\n",
        "\n",
        "# Check the layers structure\n",
        "print(\"\\nLayers structure:\")\n",
        "print(f\"Type of layers: {type(student_encoder.layers).__name__}\")\n",
        "print(f\"Length of layers: {len(student_encoder.layers)}\")\n",
        "\n",
        "# Check the first layer\n",
        "print(\"\\nFirst layer structure:\")\n",
        "print(f\"Type of first layer: {type(student_encoder.layers[0]).__name__}\")\n",
        "print(f\"Length of first layer: {len(student_encoder.layers[0])}\")\n",
        "\n",
        "# Check the structure of layers[0][0]\n",
        "print(\"\\nLayers[0][0] structure:\")\n",
        "print(f\"Type: {type(student_encoder.layers[0][0]).__name__}\")\n",
        "print(f\"Length: {len(student_encoder.layers[0][0])}\")\n",
        "\n",
        "# Check the structure of layers[0][0][0]\n",
        "print(\"\\nLayers[0][0][0] structure:\")\n",
        "for name, module in student_encoder.layers[0][0][0].named_children():\n",
        "    print(f\"  - {name}: {type(module).__name__}\")\n",
        "\n",
        "# Check if elementwise_affine is True for the layer norm\n",
        "ln_layer = student_encoder.layers[0][0][0].ln\n",
        "if hasattr(ln_layer, 'elementwise_affine'):\n",
        "    print(f\"\\nLayerNorm elementwise_affine: {ln_layer.elementwise_affine}\")\n",
        "else:\n",
        "    print(\"\\nLayerNorm does not have elementwise_affine attribute\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Alternative Way to Access Layer Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 0 keys matching 'model.student_encoder.layers.0.0.0.ln':\n",
            "\n",
            "No keys found with exact prefix. Trying a broader search...\n",
            "Found 0 keys with broader search:\n"
          ]
        }
      ],
      "source": [
        "# If the layer is a LayerNorm with elementwise_affine=False, it won't have weight/bias attributes\n",
        "# Let's try to access the layer weights through the model's state_dict instead\n",
        "\n",
        "# Get the full state_dict\n",
        "full_state_dict = model.state_dict()\n",
        "\n",
        "# Find keys related to our layer\n",
        "target_prefix = \"model.student_encoder.layers.0.0.0.ln\"\n",
        "matching_keys = [k for k in full_state_dict.keys() if k.startswith(target_prefix)]\n",
        "\n",
        "print(f\"Found {len(matching_keys)} keys matching '{target_prefix}':\")\n",
        "for key in matching_keys:\n",
        "    param = full_state_dict[key]\n",
        "    print(f\"\\nKey: {key}\")\n",
        "    print(f\"  Shape: {param.shape}\")\n",
        "    print(f\"  Type: {param.dtype}\")\n",
        "    print(f\"  First few values: {param.flatten()[:5]}\")\n",
        "    print(f\"  Mean: {param.mean().item():.6f}, Std: {param.std().item():.6f}\")\n",
        "\n",
        "# If we found matching keys, let's visualize one of them\n",
        "if matching_keys:\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        key = matching_keys[0]  # Take the first matching key\n",
        "        values = full_state_dict[key].detach().cpu().numpy().flatten()\n",
        "        \n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.hist(values, bins=50)\n",
        "        plt.title(f'Distribution of {key}')\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib not available for visualization\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating visualization: {e}\")\n",
        "else:\n",
        "    # If no keys found, try a broader search\n",
        "    print(\"\\nNo keys found with exact prefix. Trying a broader search...\")\n",
        "    broader_prefix = \"model.student_encoder\"\n",
        "    matching_keys = [k for k in full_state_dict.keys() if broader_prefix in k and \"ln\" in k and \"0.0.0\" in k]\n",
        "    \n",
        "    print(f\"Found {len(matching_keys)} keys with broader search:\")\n",
        "    for key in matching_keys:\n",
        "        param = full_state_dict[key]\n",
        "        print(f\"  {key}: {param.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "diffusion",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
